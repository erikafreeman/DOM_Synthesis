Sample_Peak_Mat <- one.sample.matrix %>% gather("sample", "value", -1) %>% filter(value > 0) %>% select(sample, peak)
##
Distance_Results <- Sample_Peak_Mat %>% left_join(Sample_Peak_Mat, by = "sample") %>% filter(peak.x > peak.y) %>% mutate(Dist = peak.x - peak.y) %>% select(sample, Dist,peak.x,peak.y)
Distance_Results$Dist.plus = Distance_Results$Dist + error.term
Distance_Results$Dist.minus = Distance_Results$Dist - error.term
Distance_Results$Trans.name = -999
head(Distance_Results)
dist.unique = unique(Distance_Results[,'sample']) #unique samples
date()
# Finding transformations which match observed mass differences (within error)
#KL note: use sapply, easier than trying to install pbapply on the HPC, FYI: slow step
mass.diff <- sapply(X = trans.full$Mass, function(x) which(Distance_Results$Dist.plus >= x & Distance_Results$Dist.minus <= x), USE.NAMES = T)
# Setting names of resulting list
names(mass.diff) = trans.full$Name
# Unlisting the new list
mass.diff = data.frame(Trans.name = rep(names(mass.diff), sapply(mass.diff, length)), Position = unlist(mass.diff)) # Transformations that don't match fall out at this step
# Setting the matching transformations
Distance_Results$Trans.name[mass.diff$Position] = as.character(mass.diff$Trans.name)
Distance_Results = Distance_Results[-which(Distance_Results$Trans.name == -999),]
head(Distance_Results)
out_dir <- "/vortexfs1/home/klongnecker/DOM_Synthesis/transformations_HPC/output_dir/"
out_dir = "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/testing/"
paste(out_dir,"Peak.2.Peak_",dist.unique,".csv",sep="")
#write.csv(Distance_Results,paste(output_dir,"/Transformation Peak Comparisons/",Sample_Name,"/Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
write.csv(Distance_Results,paste(out_dir,"Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
in_dir
out_dir
idx <-1
current.sample <- samples.to.process[idx] #KL added for testing one sample
counter = counter + 1
print(date())
one.sample.matrix = cbind(as.numeric(as.character(row.names(data))), data[,which(colnames(data) == current.sample), drop = FALSE]) # "drop = FALSE" ensures that the row and column names remain associated with the data
colnames(one.sample.matrix) = c("peak", colnames(one.sample.matrix[2]))
Sample_Peak_Mat <- one.sample.matrix %>% gather("sample", "value", -1) %>% filter(value > 0) %>% select(sample, peak)
if (nrow(Sample_Peak_Mat >= 2) & nrow(Sample_Peak_Mat) < 5000) {
##
Distance_Results <- Sample_Peak_Mat %>% left_join(Sample_Peak_Mat, by = "sample") %>% filter(peak.x > peak.y) %>% mutate(Dist = peak.x - peak.y) %>% select(sample, Dist,peak.x,peak.y)
Distance_Results$Dist.plus = Distance_Results$Dist + error.term
Distance_Results$Dist.minus = Distance_Results$Dist - error.term
Distance_Results$Trans.name = -999
head(Distance_Results)
dist.unique = unique(Distance_Results[,'sample']) #unique samples
date()
# Finding transformations which match observed mass differences (within error)
#KL note: use sapply, easier than trying to install pbapply on the HPC, FYI: slow step
mass.diff <- sapply(X = trans.full$Mass, function(x) which(Distance_Results$Dist.plus >= x & Distance_Results$Dist.minus <= x), USE.NAMES = T)
# Setting names of resulting list
names(mass.diff) = trans.full$Name
# Unlisting the new list
mass.diff = data.frame(Trans.name = rep(names(mass.diff), sapply(mass.diff, length)), Position = unlist(mass.diff)) # Transformations that don't match fall out at this step
# Setting the matching transformations
Distance_Results$Trans.name[mass.diff$Position] = as.character(mass.diff$Trans.name)
Distance_Results = Distance_Results[-which(Distance_Results$Trans.name == -999),]
head(Distance_Results)
#write.csv(Distance_Results,paste(output_dir,"/Transformation Peak Comparisons/",Sample_Name,"/Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
write.csv(Distance_Results,paste(out_dir,"Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
# Alternative .csv writing
# write.csv(Distance_Results,paste("Transformation Peak Comparisons/", "Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
# sum up the number of transformations and update the matrix
tot.trans = rbind(tot.trans,c(dist.unique,nrow(Distance_Results),nrow(Sample_Peak_Mat),nrow(Distance_Results)/nrow(Sample_Peak_Mat)))
##### write out current tot.trans in case crash
#KL turn this off for now
#  # format the total transformations matrix and write it out
#  tot.trans.out = as.data.frame(tot.trans)
#  colnames(tot.trans.out) = c('sample','total.transformations','num.of.formulas','normalized.trans')
#  tot.trans.out$sample = as.character(tot.trans.out$sample)
#  tot.trans.out$total.transformations = as.numeric(as.character(tot.trans.out$total.transformations))
# # write.csv(tot.trans.out,paste(output_dir,"/",Sample_Name,"_Total_Transformations_Temp.csv", sep=""),quote = F,row.names = F)
#  write.csv(tot.trans.out,paste(Sample_Name,"_Total_Transformations_Temp.csv", sep=""),quote = F,row.names = F)
#
#####
#pulled text that was commented out - in tforms2.R for now
print(dist.unique)
print(date())
}
print(counter)
data
# pull out just the sample names
samples.to.process = colnames(data)
head(samples.to.process)
write.table(samples.to.process,paste0("samplesToProcess",".txt"),append = FALSE, sep = "\t",row.names = FALSE,col.names=FALSE)
getwd()
dir()
setwd("~/GitHub/DOM_Synthesis/transformations_HPC/scripts_dir")
# Make sure path is set for HPC or laptop...depending on where this is getting run
args = commandArgs(trailingOnly=TRUE)
library(dplyr)
library(tidyr)
date()
paste("This is task", Sys.getenv('SLURM_ARRAY_TASK_ID'))
#this will get used later as part of folder names, just leave for now
Sample_Name = 'DOM_Syn_Trans'
#######################
### Loading in data ###
#######################
#HPC - the hard coded version (lazy)
#in_dir <- "/proj/omics/kujawinski/data/DOMsynthesis"
#out_dir <- "/vortexfs1/home/klongnecker/DOM_Synthesis/transformations_HPC/output_dir_parallel/"
#HPC - the slurm script version
in_dir <- paste0(args[1])
out_dir <- paste0(args[2])
in_dir <- "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/_data_from_2"
out_dir = "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/testing/"
# Loading in ICR data (data here: in_dir)
data = read.csv(list.files(path = in_dir,pattern = "DOM_Synthesis_Data_Trim.csv",full.names=TRUE), row.names = 1) # Keeping data and mol-data seperate to ensure they are unaltered
mol = read.csv(list.files(path = in_dir,pattern = "DOM_Synthesis_Mol_Trim",full.names=TRUE), row.names = 1)
# Loading in transformations
trans.full =  read.csv(list.files(path = in_dir,pattern= "Transformation_Database_07-2020.csv",full.names=TRUE))
trans.full$Name = as.character(trans.full$Name)
# Checking row names consistency between molecular info and data
if(identical(x = row.names(data), y = row.names(mol)) == FALSE){
stop("Something is incorrect: the mol. info and peak counts don't match")
}
# Checking to ensure ftmsRanalysis was run
if(length(which(mol$C13 == 1)) > 0){
stop("Isotopic signatures weren't removed")
}
# Probably not necessary, but checking for presence/absence
if(max(data) > 1){
print("Data was not presence/absence")
data[data > 0] = 1
}
# pull out just the sample names
samples.to.process = colnames(data)
write.table(samples.to.process,paste0("samplesToProcess",".txt"),append = FALSE, sep = "\t",row.names = FALSE,col.names=FALSE)
# Set up R script from James Stegen (PNNL) R script on transformations for an HPC
# Use this version to set this up as a set of parallel processes
#now do the transformation calculations
#KL 25 October 2023
library(dplyr)
library(tidyr)
# Load metadata object
file_list <- paste0("samplesToProcess",".txt")
files <- read.table(file = file_list,sep="\t",header=TRUE)
idx <- 2
f <- as.numeric(idx)
f
current.sample <- files$FileWithExtension[f]
head(files)
files[f,]
files[f]
current.sample <- files[f,]
# error term
error.term = 0.000010
# matrix to hold total number of transformations for each sample
tot.trans = numeric()
counter = 0
counter = counter + 1
print(date())
one.sample.matrix = cbind(as.numeric(as.character(row.names(data))), data[,which(colnames(data) == current.sample), drop = FALSE]) # "drop = FALSE" ensures that the row and column names remain associated with the data
colnames(one.sample.matrix) = c("peak", colnames(one.sample.matrix[2]))
Sample_Peak_Mat <- one.sample.matrix %>% gather("sample", "value", -1) %>% filter(value > 0) %>% select(sample, peak)
if (nrow(Sample_Peak_Mat >= 2) & nrow(Sample_Peak_Mat) < 5000) {
##
Distance_Results <- Sample_Peak_Mat %>% left_join(Sample_Peak_Mat, by = "sample") %>% filter(peak.x > peak.y) %>% mutate(Dist = peak.x - peak.y) %>% select(sample, Dist,peak.x,peak.y)
Distance_Results$Dist.plus = Distance_Results$Dist + error.term
Distance_Results$Dist.minus = Distance_Results$Dist - error.term
Distance_Results$Trans.name = -999
head(Distance_Results)
dist.unique = unique(Distance_Results[,'sample']) #unique samples
date()
# Finding transformations which match observed mass differences (within error)
#KL note: use sapply, easier than trying to install pbapply on the HPC, FYI: slow step
mass.diff <- sapply(X = trans.full$Mass, function(x) which(Distance_Results$Dist.plus >= x & Distance_Results$Dist.minus <= x), USE.NAMES = T)
# Setting names of resulting list
names(mass.diff) = trans.full$Name
# Unlisting the new list
mass.diff = data.frame(Trans.name = rep(names(mass.diff), sapply(mass.diff, length)), Position = unlist(mass.diff)) # Transformations that don't match fall out at this step
# Setting the matching transformations
Distance_Results$Trans.name[mass.diff$Position] = as.character(mass.diff$Trans.name)
Distance_Results = Distance_Results[-which(Distance_Results$Trans.name == -999),]
head(Distance_Results)
#write.csv(Distance_Results,paste(output_dir,"/Transformation Peak Comparisons/",Sample_Name,"/Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
write.csv(Distance_Results,paste(out_dir,"Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
# Alternative .csv writing
# write.csv(Distance_Results,paste("Transformation Peak Comparisons/", "Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
# sum up the number of transformations and update the matrix
#tot.trans = rbind(tot.trans,c(dist.unique,nrow(Distance_Results),nrow(Sample_Peak_Mat),nrow(Distance_Results)/nrow(Sample_Peak_Mat)))
#change this - export out one line and use that to assemble the details needed from each sample later
##### write out current tot.trans in case crash
#KL turn this off for now
#  # format the total transformations matrix and write it out
#  tot.trans.out = as.data.frame(tot.trans)
#  colnames(tot.trans.out) = c('sample','total.transformations','num.of.formulas','normalized.trans')
#  tot.trans.out$sample = as.character(tot.trans.out$sample)
#  tot.trans.out$total.transformations = as.numeric(as.character(tot.trans.out$total.transformations))
# # write.csv(tot.trans.out,paste(output_dir,"/",Sample_Name,"_Total_Transformations_Temp.csv", sep=""),quote = F,row.names = F)
#  write.csv(tot.trans.out,paste(Sample_Name,"_Total_Transformations_Temp.csv", sep=""),quote = F,row.names = F)
#
#####
#pulled text that was commented out - in tforms2.R for now
print(dist.unique)
print(date())
}
(nrow(Sample_Peak_Mat >= 2) & nrow(Sample_Peak_Mat) < 5000)
tot.trans
dist.unique
nrow(Distance_Results)
nrow(Sample_Peak_Mat)
nrow(Distance_Results)/nrow(Sample_Peak_Mat)
tot.trans <- c(dist.unique,nrow(Distance_Results),nrow(Sample_Peak_Mat),nrow(Distance_Results)/nrow(Sample_Peak_Mat))
tot.trans
#now write that to a text file
write.csv(tot.trans,paste(out_dir_summary,"Summary_",dist.unique,".csv",sep=""),quote = F,row.names = F)
out_dir_summary = "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/testing/"
#now write that to a text file
write.csv(tot.trans,paste(out_dir_summary,"Summary_",dist.unique,".csv",sep=""),quote = F,row.names = F)
#now write that to a text file
write.csv(t(tot.trans),paste(out_dir_summary,"Summary_",dist.unique,".csv",sep=""),quote = F,row.names = F)
colnames(tot.trans)
header(tot.trans)
tot.trans
t(tot.trans)
#make this one row, with headers
tot.trans <- t(tot.trans)
tot.trans
colnames(tot.trans)
colnames <- c("dist.unique","nDistance_Results","n_Sample_Peak_Mat","n_ratio")
colnames(tot.trans) <- colnames
tot.trans
#now write that to a text file
write.csv(tot.trans,paste(out_dir_summary,"Summary_",dist.unique,".csv",sep=""),quote = F,row.names = F)
dim(data)
# Load metadata object
file_list <- paste0("samplesToProcess",".txt")
files <- read.table(file = file_list,sep="\t",header=TRUE)
f <- 1
current.sample <- files[f,]
current.sample
# error term
error.term = 0.000010
# matrix to hold total number of transformations for each sample
tot.trans = numeric()
print(date())
one.sample.matrix = cbind(as.numeric(as.character(row.names(data))), data[,which(colnames(data) == current.sample), drop = FALSE]) # "drop = FALSE" ensures that the row and column names remain associated with the data
colnames(one.sample.matrix) = c("peak", colnames(one.sample.matrix[2]))
quit()
# Set up R script from James Stegen (PNNL) R script on transformations for an HPC
#KL 25 October 2023
# Make sure path is set for HPC or laptop...depending on where this is getting run
args = commandArgs(trailingOnly=TRUE)
library(dplyr)
library(tidyr)
date()
paste("This is task", Sys.getenv('SLURM_ARRAY_TASK_ID'))
#this will get used later as part of folder names, just leave for now
Sample_Name = 'DOM_Syn_Trans'
in_dir <- "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/_data_from_2"
out_dir = "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/testing/"
# Loading in ICR data (data here: in_dir)
data = read.csv(list.files(path = in_dir,pattern = "DOM_Synthesis_Data_Trim.csv",full.names=TRUE), row.names = 1) # Keeping data and mol-data seperate to ensure they are unaltered
mol = read.csv(list.files(path = in_dir,pattern = "DOM_Synthesis_Mol_Trim",full.names=TRUE), row.names = 1)
# Loading in transformations
trans.full =  read.csv(list.files(path = in_dir,pattern= "Transformation_Database_07-2020.csv",full.names=TRUE))
trans.full$Name = as.character(trans.full$Name)
# Checking row names consistency between molecular info and data
if(identical(x = row.names(data), y = row.names(mol)) == FALSE){
stop("Something is incorrect: the mol. info and peak counts don't match")
}
# Checking to ensure ftmsRanalysis was run
if(length(which(mol$C13 == 1)) > 0){
stop("Isotopic signatures weren't removed")
}
# Probably not necessary, but checking for presence/absence
if(max(data) > 1){
print("Data was not presence/absence")
data[data > 0] = 1
}
# pull out just the sample names
samples.to.process = colnames(data)
samples.to.process[515:523]
View(samples.to.process)
samples.to.process[500:550]
samples.to.process[550]
samples.to.process[547]
getwd()
ls *txt
# pull out just the sample names
samples.to.process = colnames(data)
write.table(samples.to.process,paste0("samplesToProcess",".txt"),append = FALSE, sep = "\t",row.names = FALSE,col.names=FALSE)
ls *txt
dir()
# Load metadata object
file_list <- paste0("samplesToProcess",".txt")
files <- read.table(file = file_list,sep="\t",header=TRUE)
#make a matrix and export the file...testing
forExport <- c(in_dir,out_dir,current.sample)
f<-1
current.sample <- files[f,]
#make a matrix and export the file...testing
forExport <- c(in_dir,out_dir,current.sample)
forExport
write.csv(forExport,paste("testing.csv",sep=""),quote = F,row.names = F)
getwd()
ls -l
dir()
#laptop, local trouble shooting
#in_dir <- "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/_data_from_2"
out_dir = "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/testing/"
############
# read samplesToProcess.txt - that will serve to compare that I have all the one-liners
# where did I put that? (in scripts_dir, not the best place but fine)
file_list <- paste0("samplesToProcess",".txt")
rm(ls=str.())
rm(ls=str())
rm(list=ls(all=T))
library(dplyr)
library(tidyr)
date()
paste("This is task", Sys.getenv('SLURM_ARRAY_TASK_ID'))
#laptop, local trouble shooting
#in_dir <- "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/_data_from_2"
out_dir = "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/testing/"
############
# read samplesToProcess.txt - that will serve to compare that I have all the one-liners
# where did I put that? (in scripts_dir, not the best place but fine)
file_list <- paste0("samplesToProcess",".txt")
files <- read.table(file = file_list,sep="\t",header=TRUE)
# get the full list of files in a directory, files are all in out_dir and begin with 'Summary'
sumList <- list.files(out_dir,pattern = "Summary_")
setwd(out_dir)
ls
dir()
list.files(pattern ="Summary_")
setwd("~/GitHub/DOM_Synthesis/transformations_HPC/scripts_dir")
#laptop, local trouble shooting
#in_dir <- "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/_data_from_2"
out_dir = "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/testing/"
# get the full list of files in a directory, files are all in out_dir and begin with 'Summary'
sumList <- list.files(out_dir,pattern = "Summary_")
length(sumList)
sumList[1]
help("read.csv")
#first, make a place to put the one liners, can start with the first file
one <- read.csv(sumList[1],header=FALSE)
#first, make a place to put the one liners, can start with the first file
one <- read.csv(list.files(out_dir,sumList[1]),header=FALSE)
#first, make a place to put the one liners, can start with the first file
one <- read.csv(list.files(path = out_dir,sumList[1],full.names=TRUE),header=FALSE)
one
dim(one)
one[2,]
tot.trans = one[2,]
rm(one)
source("~/GitHub/DOM_Synthesis/transformations_HPC/scripts_dir/tforms_parallel_step3.R")
#then export the result as a single CSV file
write.csv(tot.trans,paste0(out_dir,"Transformulas_fullSet",".csv",sep=""),quote = F,row.names = F)
source("~/GitHub/DOM_Synthesis/transformations_HPC/scripts_dir/tforms_parallel_step3.R")
#then export the result as a single CSV file
write.csv(tot.trans,paste0("Transformulas_fullSet",".csv",sep=""),quote = F,row.names = F)
#first, make a place to put the one liners, can start with the first file
one <- read.csv(list.files(path = out_dir,sumList[1],full.names=TRUE),header=TRUE)
one
tot.trans = one[1,]
tot.trans
source("~/GitHub/DOM_Synthesis/transformations_HPC/scripts_dir/tforms_parallel_step3.R")
source("~/GitHub/DOM_Synthesis/transformations_HPC/scripts_dir/tforms_parallel_step3.R")
rm(list=ls(all=T))
library(dplyr)
library(tidyr)
date()
paste("This is task", Sys.getenv('SLURM_ARRAY_TASK_ID'))
#laptop, local trouble shooting
#in_dir <- "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/_data_from_2"
out_dir = "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/testing/"
############
# read samplesToProcess.txt - that will serve to compare that I have all the one-liners
# where did I put that? (in scripts_dir, not the best place but fine)
file_list <- paste0("samplesToProcess",".txt")
files <- read.table(file = file_list,sep="\t",header=TRUE)
# get the full list of files in a directory, files are all in out_dir and begin with 'Summary'
sumList <- list.files(out_dir,pattern = "Summary_")
#first, make a place to put the one liners, can start with the first file
one <- read.csv(list.files(path = out_dir,sumList[1],full.names=TRUE),header=TRUE)
tot.trans = one[1,]
rm(one)
#start idx at 2 because used first file as base of matrix
for (idx in 2:length(sumList)) {
#read the file and append to running list
one <- read.csv(list.files(path = out_dir,sumList[idx],full.names=TRUE),header=FALSE)
tot.trans = rbind(tot.trans,one[2,])
rm(one)
}
idx <-2
#read the file and append to running list
one <- read.csv(list.files(path = out_dir,sumList[idx],full.names=TRUE),header=FALSE)
tot.trans = rbind(tot.trans,one[1,])
tot.trans = rbind(tot.trans,one)
one
tot.trans
help("rbind")
tot.trans = rbind(tot.trans,one,make.row.names=FALSE)
tot.trans
one
one[1,]
tot.trans[idx,] = one[1,]
tot.trans
tot.trans[idx,] = one[2,]
#first, make a place to put the one liners, can start with the first file
one <- read.csv(list.files(path = out_dir,sumList[1],full.names=TRUE),header=TRUE)
tot.trans = one[1,]
#read the file and append to running list
one <- read.csv(list.files(path = out_dir,sumList[idx],full.names=TRUE),header=FALSE)
tot.trans[idx,] = one[2,]
tot.trans
source("~/GitHub/DOM_Synthesis/transformations_HPC/scripts_dir/tforms_parallel_step3.R")
length(files)
dim(files)
dim(sumList)
length(sumList)
##put in an error check here - do I have the same number of files as in the original list?
nList = dim(files)[1]
nFound = length(sumList)
help(isequal)
(!nList ~=nFound)
(nList ==nFound)
nList
!(nList ==nFound)
if!(nList == nFound){
if(!nList == nFound){
stop("Something is incorrect: the list of one-liners does not match the original list of samples")
}
nList
nList <- 5
if(!nList == nFound){
stop("Something is incorrect: the list of one-liners does not match the original list of samples")
}
sumList
# Set up R script from James Stegen (PNNL) R script on transformations for an HPC
# Use this version to set this up as a set of parallel processes
# step2: now do the transformation calculations
#KL 25 October 2023
library(dplyr)
library(tidyr)
args = commandArgs(trailingOnly=TRUE) #remember need this to use the args from the slurm script
out_dir = "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/testing/"
out_dir_summary = "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/testing2/"
# Loading in ICR data (data here: in_dir)
data = read.csv(list.files(path = in_dir,pattern = "DOM_Synthesis_Data_Trim.csv",full.names=TRUE), row.names = 1) # Keeping data and mol-data seperate to ensure they are unaltered
mol = read.csv(list.files(path = in_dir,pattern = "DOM_Synthesis_Mol_Trim",full.names=TRUE), row.names = 1)
# Loading in transformations
trans.full =  read.csv(list.files(path = in_dir,pattern= "Transformation_Database_07-2020.csv",full.names=TRUE))
trans.full$Name = as.character(trans.full$Name)
in_dir <- "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/_data_from_2"
# Loading in ICR data (data here: in_dir)
data = read.csv(list.files(path = in_dir,pattern = "DOM_Synthesis_Data_Trim.csv",full.names=TRUE), row.names = 1) # Keeping data and mol-data seperate to ensure they are unaltered
mol = read.csv(list.files(path = in_dir,pattern = "DOM_Synthesis_Mol_Trim",full.names=TRUE), row.names = 1)
# Loading in transformations
trans.full =  read.csv(list.files(path = in_dir,pattern= "Transformation_Database_07-2020.csv",full.names=TRUE))
trans.full$Name = as.character(trans.full$Name)
# Load metadata object
file_list <- paste0("samplesToProcess",".txt")
files <- read.table(file = file_list,sep="\t",header=TRUE)
# File to process based on array number
f<- as.numeric(paste0(args[4]))
f <- 2
current.sample <- files[f,]
# error term
error.term = 0.000010
# matrix to hold total number of transformations for each sample
tot.trans = numeric()
print(date())
one.sample.matrix = cbind(as.numeric(as.character(row.names(data))), data[,which(colnames(data) == current.sample), drop = FALSE]) # "drop = FALSE" ensures that the row and column names remain associated with the data
colnames(one.sample.matrix) = c("peak", colnames(one.sample.matrix[2]))
Sample_Peak_Mat <- one.sample.matrix %>% gather("sample", "value", -1) %>% filter(value > 0) %>% select(sample, peak)
if (nrow(Sample_Peak_Mat >= 2) & nrow(Sample_Peak_Mat) < 5000) {
##
Distance_Results <- Sample_Peak_Mat %>% left_join(Sample_Peak_Mat, by = "sample") %>% filter(peak.x > peak.y) %>% mutate(Dist = peak.x - peak.y) %>% select(sample, Dist,peak.x,peak.y)
Distance_Results$Dist.plus = Distance_Results$Dist + error.term
Distance_Results$Dist.minus = Distance_Results$Dist - error.term
Distance_Results$Trans.name = -999
head(Distance_Results)
dist.unique = unique(Distance_Results[,'sample']) #unique samples
date()
# Finding transformations which match observed mass differences (within error)
#KL note: use sapply, easier than trying to install pbapply on the HPC, FYI: slow step
mass.diff <- sapply(X = trans.full$Mass, function(x) which(Distance_Results$Dist.plus >= x & Distance_Results$Dist.minus <= x), USE.NAMES = T)
# Setting names of resulting list
names(mass.diff) = trans.full$Name
# Unlisting the new list
mass.diff = data.frame(Trans.name = rep(names(mass.diff), sapply(mass.diff, length)), Position = unlist(mass.diff)) # Transformations that don't match fall out at this step
# Setting the matching transformations
Distance_Results$Trans.name[mass.diff$Position] = as.character(mass.diff$Trans.name)
Distance_Results = Distance_Results[-which(Distance_Results$Trans.name == -999),]
head(Distance_Results)
#write.csv(Distance_Results,paste(output_dir,"/Transformation Peak Comparisons/",Sample_Name,"/Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
write.csv(Distance_Results,paste(out_dir,"Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
# sum up the number of transformations and update the matrix
#tot.trans = rbind(tot.trans,c(dist.unique,nrow(Distance_Results),nrow(Sample_Peak_Mat),nrow(Distance_Results)/nrow(Sample_Peak_Mat)))
#change this - export out one line and use that to assemble the details needed from each sample later
tot.trans <- c(dist.unique,nrow(Distance_Results),nrow(Sample_Peak_Mat),nrow(Distance_Results)/nrow(Sample_Peak_Mat))
#make this one row, with headers
tot.trans <- t(tot.trans)
colnames <- c("dist.unique","nDistance_Results","n_Sample_Peak_Mat","n_ratio")
colnames(tot.trans) <- colnames
#now write that to a text file
write.csv(tot.trans,paste(out_dir_summary,"Summary_",dist.unique,".csv",sep=""),quote = F,row.names = F)
}
View(samples.to.process)
# pull out just the sample names
samples.to.process = colnames(data)
samples.to.process[1480:1500]
View(data)
parse <- data[,[1400:1500]]
parse <- data[,1400:1500]
View(parse)
fi <-"ManyFiles_MGC1903260_FTMS_Lakes_FJ_Sweden_43_112_01_22816.corems"
which(fi,samples.to.process)
samples.to.process[samples.to.process == fi]
which(samples.to.process==fi,arr.ind=TRUE)
samples.to.process[740:744]
quit()
