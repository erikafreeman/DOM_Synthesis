# suppressWarnings({
# mData <- read_excel(fName)
# })
mData <- read.xlsx(fName)
# #read in the metadata, but shut down the errors
# suppressWarnings({
# mData <- read_excel(fName)
# })
mData <- read.xlsx(fName,1)
View(mData)
#only need a few columns, so pull in manually
idxRows <- c(1:20) #this will work...but definitely only numbers
#idxRows <- c(1:dim(mData)[1]) #how many rows...
cols <- colnames(mData)
idxCol =c('C_pool','N_value')
t <- mData[idxRows,idxCols]
idxCol =c('C_pool','N_value')
t <- mData[idxRows,idxCols]
t <- mData[idxRows,idxCol]
typeof(t)
View(t)
# #read in the metadata, but shut down the errors
# suppressWarnings({
# mData <- read_excel(fName)
# })
mData <- read.xlsx(fName,1,as.data.frame=TRUE)
#for now, make a dummy dataset for NMS using the environmental variables in #the metadata folder
#only need a few columns, so pull in manually
idxRows <- c(1:20) #this will work...but definitely only numbers
#idxRows <- c(1:dim(mData)[1]) #how many rows...
cols <- colnames(mData)
idxCol =c('C_pool','N_value')
#need unlist because the first bit produces a list
useData <- as.numeric(unlist(mData[idxRows,idxCol]))
pretzel <- mData[idxRows,idxCol]
typeof(pretzel)
# #read in the metadata, but shut down the errors
# suppressWarnings({
# mData <- read_excel(fName)
# })
mData <- read.xlsx2(fName,1,as.data.frame=TRUE)
#for now, make a dummy dataset for NMS using the environmental variables in #the metadata folder
#only need a few columns, so pull in manually
idxRows <- c(1:20) #this will work...but definitely only numbers
#idxRows <- c(1:dim(mData)[1]) #how many rows...
cols <- colnames(mData)
idxCol =c('C_pool','N_value')
#need unlist because the first bit produces a list
useData <- as.numeric(unlist(mData[idxRows,idxCol]))
pretzel <- mData[idxRows,idxCol]
typeof(pretzel)
# #read in the metadata, but shut down the errors
# suppressWarnings({
# mData <- read_excel(fName)
# })
mData <- read.xlsx2(fName,1,as.data.frame=FALSE)
View(mData)
# #read in the metadata, but shut down the errors
# suppressWarnings({
# mData <- read_excel(fName)
# })
mData <- read.xlsx2(fName,1,as.data.frame=TRUE)
#for now, make a dummy dataset for NMS using the environmental variables in #the metadata folder
#only need a few columns, so pull in manually
idxRows <- c(1:20) #this will work...but definitely only numbers
#idxRows <- c(1:dim(mData)[1]) #how many rows...
cols <- colnames(mData)
idxCol =c('C_pool','N_value')
#need unlist because the first bit produces a list
useData <- as.numeric(unlist(mData[idxRows,idxCol]))
pretzel <- mData[idxRows,idxCol]
g <-as.data.frame(do.call(cbind, pretzel))
typeof(g)
df <- data.frame(t(sapply(pretzel,c)))
d <- as.data.frame(do.call(rbind, pretzel))
install.packages("data.table")
library(data.table)
help(data.table)
help("setDT")
f4 <- setDT(pretzel,keep.rownames=TRUE)
typeof(f4)
useData <- f4
#data pre-processing; #requires the vegan package
dataRel = decostand(x=useData,method = "pa",na.rm=TRUE)
r = vegdist(dataRel,method = "bray",na.rm=TRUE)
sampleTree = flashClust(r,method = "ward")
par(cex=1.5)
par(mar = c(2,2,2,3))
plot(as.dendrogram(sampleTree),horiz=T)
#for now, make a dummy dataset for NMS using the environmental variables in #the metadata folder
#only need a few columns, so pull in manually
#idxRows <- c(1:20) #this will work...but definitely only numbers
idxRows <- c(1:dim(mData)[1]) #how many rows...
cols <- colnames(mData)
idxCol =c('C_pool','N_value')
#need unlist because the first bit produces a list
useData <- as.numeric(unlist(mData[idxRows,idxCol]))
pretzel <- mData[idxRows,idxCol]
df <- data.frame(t(sapply(pretzel,c)))
d <- as.data.frame(do.call(rbind, pretzel))
f4 <- setDT(pretzel,keep.rownames=TRUE)
useData <- f4
#data pre-processing; #requires the vegan package
dataRel = decostand(x=useData,method = "pa",na.rm=TRUE)
r = vegdist(dataRel,method = "bray",na.rm=TRUE)
sampleTree = flashClust(r,method = "ward")
par(cex=1.5)
par(mar = c(2,2,2,3))
plot(as.dendrogram(sampleTree),horiz=T)
f4r <- as.data.table(pretzel)
useData <- f4r
#data pre-processing; #requires the vegan package
dataRel = decostand(x=useData,method = "pa",na.rm=TRUE)
r = vegdist(dataRel,method = "bray",na.rm=TRUE)
sampleTree = flashClust(r,method = "ward")
par(cex=1.5)
par(mar = c(2,2,2,3))
plot(as.dendrogram(sampleTree),horiz=T)
f4r <- as.data.table(pretzel)
useData <- f4r
help("randu")
#add some random numbers to additional columns for now
useData[,3]<-runif(n=10, min=1, max=20)
dim(useData)[1]
#add some random numbers to additional columns for now
useData[,3]<-runif(n=dim(useData)[1], min=1, max=1e5)
useData[,5]<-runif(n=dim(useData)[1], min=1, max=1e5)
useData[,4]<-runif(n=dim(useData)[1], min=1, max=1e5)
useData[,5]<-runif(n=dim(useData)[1], min=1, max=1e5)
useData <- f4
#add some random numbers to additional columns for now
useData[,3]<-runif(n=dim(useData)[1], min=1, max=1e5)
useData[,4]<-runif(n=dim(useData)[1], min=1, max=1e5)
useData <- f4
#add some random numbers to additional columns for now
useData[,3:5]<-runif(n=dim(useData)[1], min=1, max=1e5)
dim(useData)[1]
useData <- f4
#add some random numbers to additional columns for now
useData['random1',3]<-runif(n=dim(useData)[1], min=1, max=1e5)
useData <- f4
useData <- f4
#add some random numbers to additional columns for now
useData[,'random1']<-runif(n=dim(useData)[1], min=1, max=1e5)
useData[,'random2',]<-runif(n=dim(useData)[1], min=1, max=1e5)
useData[,'random2']<-runif(n=dim(useData)[1], min=1, max=1e5)
useData[,'random3']<-runif(n=dim(useData)[1], min=1, max=1e5)
install.packages("geosphere")
library(geosphere)
help("metaMDS")
# Set up R script from James Stegen (PNNL) R script on transformations for an HPC
#KL 25 October 2023
# Make sure path is set for HPC or laptop...depending on where this is getting run
args = commandArgs(trailingOnly=TRUE)
library(dplyr)
library(tidyr)
date()
paste("This is task", Sys.getenv('SLURM_ARRAY_TASK_ID'))
#this will get used later as part of folder names, just leave for now
Sample_Name = 'DOM_Syn_Trans'
#######################
### Loading in data ###
#######################
#HPC - the hard coded version
in_dir <- "/proj/omics/kujawinski/data/DOMsynthesis"
out_dir = getwd()
out_dir
# Loading in ICR data (data are in dataPath)
data = read.csv(list.files(path = in_dir,pattern = "DOM_Synthesis_Data_Trim.csv",full.names=TRUE), row.names = 1) # Keeping data and mol-data seperate to ensure they are unaltered
mol = read.csv(list.files(path = in_dir,pattern = "DOM_Synthesis_Mol_Trim",full.names=TRUE), row.names = 1)
# Loading in transformations
trans.full =  read.csv(list.files(path = in_dir,pattern= "Transformation_Database_07-2020.csv"))
trans.full$Name = as.character(trans.full$Name)
# Checking row names consistency between molecular info and data
if(identical(x = row.names(data), y = row.names(mol)) == FALSE){
stop("Something is incorrect: the mol. info and peak counts don't match")
}
# Checking to ensure ftmsRanalysis was run
if(length(which(mol$C13 == 1)) > 0){
stop("Isotopic signatures weren't removed")
}
# Probably not necessary, but checking for presence/absence
if(max(data) > 1){
print("Data was not presence/absence")
data[data > 0] = 1
}
# pull out just the sample names
samples.to.process = colnames(data)
# error term
error.term = 0.000010
# matrix to hold total number of transformations for each sample
tot.trans = numeric()
counter = 0
#from James (testing), this sample will work: Behnke2022_2020February20NegESI_Fen_OiL_0724_i.corems
idx <- 2
current.sample <- samples.to.process[idx] #KL added for testing one sample
counter = counter + 1
print(date())
one.sample.matrix = cbind(as.numeric(as.character(row.names(data))), data[,which(colnames(data) == current.sample), drop = FALSE]) # "drop = FALSE" ensures that the row and column names remain associated with the data
colnames(one.sample.matrix) = c("peak", colnames(one.sample.matrix[2]))
Sample_Peak_Mat <- one.sample.matrix %>% gather("sample", "value", -1) %>% filter(value > 0) %>% select(sample, peak)
if (nrow(Sample_Peak_Mat >= 2) & nrow(Sample_Peak_Mat) < 5000) {
##
Distance_Results <- Sample_Peak_Mat %>% left_join(Sample_Peak_Mat, by = "sample") %>% filter(peak.x > peak.y) %>% mutate(Dist = peak.x - peak.y) %>% select(sample, Dist,peak.x,peak.y)
Distance_Results$Dist.plus = Distance_Results$Dist + error.term
Distance_Results$Dist.minus = Distance_Results$Dist - error.term
Distance_Results$Trans.name = -999
head(Distance_Results)
dist.unique = unique(Distance_Results[,'sample']) #unique samples
date()
# Finding transformations which match observed mass differences (within error)
#KL note: use sapply, easier than trying to install pbapply on the HPC, FYI: slow step
mass.diff <- sapply(X = trans.full$Mass, function(x) which(Distance_Results$Dist.plus >= x & Distance_Results$Dist.minus <= x), USE.NAMES = T)
# Setting names of resulting list
names(mass.diff) = trans.full$Name
# Unlisting the new list
mass.diff = data.frame(Trans.name = rep(names(mass.diff), sapply(mass.diff, length)), Position = unlist(mass.diff)) # Transformations that don't match fall out at this step
# Setting the matching transformations
Distance_Results$Trans.name[mass.diff$Position] = as.character(mass.diff$Trans.name)
Distance_Results = Distance_Results[-which(Distance_Results$Trans.name == -999),]
head(Distance_Results)
#write.csv(Distance_Results,paste(output_dir,"/Transformation Peak Comparisons/",Sample_Name,"/Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
write.csv(Distance_Results,paste(out_dir,"Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
# Alternative .csv writing
# write.csv(Distance_Results,paste("Transformation Peak Comparisons/", "Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
# sum up the number of transformations and update the matrix
tot.trans = rbind(tot.trans,c(dist.unique,nrow(Distance_Results),nrow(Sample_Peak_Mat),nrow(Distance_Results)/nrow(Sample_Peak_Mat)))
##### write out current tot.trans in case crash
#KL turn this off for now
#  # format the total transformations matrix and write it out
#  tot.trans.out = as.data.frame(tot.trans)
#  colnames(tot.trans.out) = c('sample','total.transformations','num.of.formulas','normalized.trans')
#  tot.trans.out$sample = as.character(tot.trans.out$sample)
#  tot.trans.out$total.transformations = as.numeric(as.character(tot.trans.out$total.transformations))
# # write.csv(tot.trans.out,paste(output_dir,"/",Sample_Name,"_Total_Transformations_Temp.csv", sep=""),quote = F,row.names = F)
#  write.csv(tot.trans.out,paste(Sample_Name,"_Total_Transformations_Temp.csv", sep=""),quote = F,row.names = F)
#
#####
#pulled text that was commented out - in tforms2.R for now
print(dist.unique)
print(date())
}
print(counter)
idx
current.sample
samples.to.process
in_dir <- "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/_data_from_2"
# Loading in ICR data (data are in dataPath)
data = read.csv(list.files(path = in_dir,pattern = "DOM_Synthesis_Data_Trim.csv",full.names=TRUE), row.names = 1) # Keeping data and mol-data seperate to ensure they are unaltered
mol = read.csv(list.files(path = in_dir,pattern = "DOM_Synthesis_Mol_Trim",full.names=TRUE), row.names = 1)
# Loading in transformations
trans.full =  read.csv(list.files(path = in_dir,pattern= "Transformation_Database_07-2020.csv"))
in_dire
in_dir
# Loading in transformations
trans.full =  read.csv(list.files(path = in_dir,pattern= "Transformation_Database_07-2020.csv",full.names=TRUE))
trans.full$Name = as.character(trans.full$Name)
# Set up R script from James Stegen (PNNL) R script on transformations for an HPC
#KL 25 October 2023
# Make sure path is set for HPC or laptop...depending on where this is getting run
args = commandArgs(trailingOnly=TRUE)
library(dplyr)
library(tidyr)
# pull out just the sample names
samples.to.process = colnames(data)
# error term
error.term = 0.000010
# matrix to hold total number of transformations for each sample
tot.trans = numeric()
counter = 0
current.sample <- samples.to.process[idx] #KL added for testing one sample
counter = counter + 1
print(date())
one.sample.matrix = cbind(as.numeric(as.character(row.names(data))), data[,which(colnames(data) == current.sample), drop = FALSE]) # "drop = FALSE" ensures that the row and column names remain associated with the data
colnames(one.sample.matrix) = c("peak", colnames(one.sample.matrix[2]))
Sample_Peak_Mat <- one.sample.matrix %>% gather("sample", "value", -1) %>% filter(value > 0) %>% select(sample, peak)
##
Distance_Results <- Sample_Peak_Mat %>% left_join(Sample_Peak_Mat, by = "sample") %>% filter(peak.x > peak.y) %>% mutate(Dist = peak.x - peak.y) %>% select(sample, Dist,peak.x,peak.y)
Distance_Results$Dist.plus = Distance_Results$Dist + error.term
Distance_Results$Dist.minus = Distance_Results$Dist - error.term
Distance_Results$Trans.name = -999
head(Distance_Results)
dist.unique = unique(Distance_Results[,'sample']) #unique samples
date()
# Finding transformations which match observed mass differences (within error)
#KL note: use sapply, easier than trying to install pbapply on the HPC, FYI: slow step
mass.diff <- sapply(X = trans.full$Mass, function(x) which(Distance_Results$Dist.plus >= x & Distance_Results$Dist.minus <= x), USE.NAMES = T)
# Setting names of resulting list
names(mass.diff) = trans.full$Name
# Unlisting the new list
mass.diff = data.frame(Trans.name = rep(names(mass.diff), sapply(mass.diff, length)), Position = unlist(mass.diff)) # Transformations that don't match fall out at this step
# Setting the matching transformations
Distance_Results$Trans.name[mass.diff$Position] = as.character(mass.diff$Trans.name)
Distance_Results = Distance_Results[-which(Distance_Results$Trans.name == -999),]
head(Distance_Results)
out_dir <- "/vortexfs1/home/klongnecker/DOM_Synthesis/transformations_HPC/output_dir/"
out_dir = "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/testing/"
paste(out_dir,"Peak.2.Peak_",dist.unique,".csv",sep="")
#write.csv(Distance_Results,paste(output_dir,"/Transformation Peak Comparisons/",Sample_Name,"/Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
write.csv(Distance_Results,paste(out_dir,"Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
in_dir
out_dir
idx <-1
current.sample <- samples.to.process[idx] #KL added for testing one sample
counter = counter + 1
print(date())
one.sample.matrix = cbind(as.numeric(as.character(row.names(data))), data[,which(colnames(data) == current.sample), drop = FALSE]) # "drop = FALSE" ensures that the row and column names remain associated with the data
colnames(one.sample.matrix) = c("peak", colnames(one.sample.matrix[2]))
Sample_Peak_Mat <- one.sample.matrix %>% gather("sample", "value", -1) %>% filter(value > 0) %>% select(sample, peak)
if (nrow(Sample_Peak_Mat >= 2) & nrow(Sample_Peak_Mat) < 5000) {
##
Distance_Results <- Sample_Peak_Mat %>% left_join(Sample_Peak_Mat, by = "sample") %>% filter(peak.x > peak.y) %>% mutate(Dist = peak.x - peak.y) %>% select(sample, Dist,peak.x,peak.y)
Distance_Results$Dist.plus = Distance_Results$Dist + error.term
Distance_Results$Dist.minus = Distance_Results$Dist - error.term
Distance_Results$Trans.name = -999
head(Distance_Results)
dist.unique = unique(Distance_Results[,'sample']) #unique samples
date()
# Finding transformations which match observed mass differences (within error)
#KL note: use sapply, easier than trying to install pbapply on the HPC, FYI: slow step
mass.diff <- sapply(X = trans.full$Mass, function(x) which(Distance_Results$Dist.plus >= x & Distance_Results$Dist.minus <= x), USE.NAMES = T)
# Setting names of resulting list
names(mass.diff) = trans.full$Name
# Unlisting the new list
mass.diff = data.frame(Trans.name = rep(names(mass.diff), sapply(mass.diff, length)), Position = unlist(mass.diff)) # Transformations that don't match fall out at this step
# Setting the matching transformations
Distance_Results$Trans.name[mass.diff$Position] = as.character(mass.diff$Trans.name)
Distance_Results = Distance_Results[-which(Distance_Results$Trans.name == -999),]
head(Distance_Results)
#write.csv(Distance_Results,paste(output_dir,"/Transformation Peak Comparisons/",Sample_Name,"/Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
write.csv(Distance_Results,paste(out_dir,"Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
# Alternative .csv writing
# write.csv(Distance_Results,paste("Transformation Peak Comparisons/", "Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
# sum up the number of transformations and update the matrix
tot.trans = rbind(tot.trans,c(dist.unique,nrow(Distance_Results),nrow(Sample_Peak_Mat),nrow(Distance_Results)/nrow(Sample_Peak_Mat)))
##### write out current tot.trans in case crash
#KL turn this off for now
#  # format the total transformations matrix and write it out
#  tot.trans.out = as.data.frame(tot.trans)
#  colnames(tot.trans.out) = c('sample','total.transformations','num.of.formulas','normalized.trans')
#  tot.trans.out$sample = as.character(tot.trans.out$sample)
#  tot.trans.out$total.transformations = as.numeric(as.character(tot.trans.out$total.transformations))
# # write.csv(tot.trans.out,paste(output_dir,"/",Sample_Name,"_Total_Transformations_Temp.csv", sep=""),quote = F,row.names = F)
#  write.csv(tot.trans.out,paste(Sample_Name,"_Total_Transformations_Temp.csv", sep=""),quote = F,row.names = F)
#
#####
#pulled text that was commented out - in tforms2.R for now
print(dist.unique)
print(date())
}
print(counter)
data
# pull out just the sample names
samples.to.process = colnames(data)
head(samples.to.process)
write.table(samples.to.process,paste0("samplesToProcess",".txt"),append = FALSE, sep = "\t",row.names = FALSE,col.names=FALSE)
getwd()
dir()
setwd("~/GitHub/DOM_Synthesis/transformations_HPC/scripts_dir")
# Make sure path is set for HPC or laptop...depending on where this is getting run
args = commandArgs(trailingOnly=TRUE)
library(dplyr)
library(tidyr)
date()
paste("This is task", Sys.getenv('SLURM_ARRAY_TASK_ID'))
#this will get used later as part of folder names, just leave for now
Sample_Name = 'DOM_Syn_Trans'
#######################
### Loading in data ###
#######################
#HPC - the hard coded version (lazy)
#in_dir <- "/proj/omics/kujawinski/data/DOMsynthesis"
#out_dir <- "/vortexfs1/home/klongnecker/DOM_Synthesis/transformations_HPC/output_dir_parallel/"
#HPC - the slurm script version
in_dir <- paste0(args[1])
out_dir <- paste0(args[2])
in_dir <- "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/_data_from_2"
out_dir = "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/testing/"
# Loading in ICR data (data here: in_dir)
data = read.csv(list.files(path = in_dir,pattern = "DOM_Synthesis_Data_Trim.csv",full.names=TRUE), row.names = 1) # Keeping data and mol-data seperate to ensure they are unaltered
mol = read.csv(list.files(path = in_dir,pattern = "DOM_Synthesis_Mol_Trim",full.names=TRUE), row.names = 1)
# Loading in transformations
trans.full =  read.csv(list.files(path = in_dir,pattern= "Transformation_Database_07-2020.csv",full.names=TRUE))
trans.full$Name = as.character(trans.full$Name)
# Checking row names consistency between molecular info and data
if(identical(x = row.names(data), y = row.names(mol)) == FALSE){
stop("Something is incorrect: the mol. info and peak counts don't match")
}
# Checking to ensure ftmsRanalysis was run
if(length(which(mol$C13 == 1)) > 0){
stop("Isotopic signatures weren't removed")
}
# Probably not necessary, but checking for presence/absence
if(max(data) > 1){
print("Data was not presence/absence")
data[data > 0] = 1
}
# pull out just the sample names
samples.to.process = colnames(data)
write.table(samples.to.process,paste0("samplesToProcess",".txt"),append = FALSE, sep = "\t",row.names = FALSE,col.names=FALSE)
# Set up R script from James Stegen (PNNL) R script on transformations for an HPC
# Use this version to set this up as a set of parallel processes
#now do the transformation calculations
#KL 25 October 2023
library(dplyr)
library(tidyr)
# Load metadata object
file_list <- paste0("samplesToProcess",".txt")
files <- read.table(file = file_list,sep="\t",header=TRUE)
idx <- 2
f <- as.numeric(idx)
f
current.sample <- files$FileWithExtension[f]
head(files)
files[f,]
files[f]
current.sample <- files[f,]
# error term
error.term = 0.000010
# matrix to hold total number of transformations for each sample
tot.trans = numeric()
counter = 0
counter = counter + 1
print(date())
one.sample.matrix = cbind(as.numeric(as.character(row.names(data))), data[,which(colnames(data) == current.sample), drop = FALSE]) # "drop = FALSE" ensures that the row and column names remain associated with the data
colnames(one.sample.matrix) = c("peak", colnames(one.sample.matrix[2]))
Sample_Peak_Mat <- one.sample.matrix %>% gather("sample", "value", -1) %>% filter(value > 0) %>% select(sample, peak)
if (nrow(Sample_Peak_Mat >= 2) & nrow(Sample_Peak_Mat) < 5000) {
##
Distance_Results <- Sample_Peak_Mat %>% left_join(Sample_Peak_Mat, by = "sample") %>% filter(peak.x > peak.y) %>% mutate(Dist = peak.x - peak.y) %>% select(sample, Dist,peak.x,peak.y)
Distance_Results$Dist.plus = Distance_Results$Dist + error.term
Distance_Results$Dist.minus = Distance_Results$Dist - error.term
Distance_Results$Trans.name = -999
head(Distance_Results)
dist.unique = unique(Distance_Results[,'sample']) #unique samples
date()
# Finding transformations which match observed mass differences (within error)
#KL note: use sapply, easier than trying to install pbapply on the HPC, FYI: slow step
mass.diff <- sapply(X = trans.full$Mass, function(x) which(Distance_Results$Dist.plus >= x & Distance_Results$Dist.minus <= x), USE.NAMES = T)
# Setting names of resulting list
names(mass.diff) = trans.full$Name
# Unlisting the new list
mass.diff = data.frame(Trans.name = rep(names(mass.diff), sapply(mass.diff, length)), Position = unlist(mass.diff)) # Transformations that don't match fall out at this step
# Setting the matching transformations
Distance_Results$Trans.name[mass.diff$Position] = as.character(mass.diff$Trans.name)
Distance_Results = Distance_Results[-which(Distance_Results$Trans.name == -999),]
head(Distance_Results)
#write.csv(Distance_Results,paste(output_dir,"/Transformation Peak Comparisons/",Sample_Name,"/Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
write.csv(Distance_Results,paste(out_dir,"Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
# Alternative .csv writing
# write.csv(Distance_Results,paste("Transformation Peak Comparisons/", "Peak.2.Peak_",dist.unique,".csv",sep=""),quote = F,row.names = F)
# sum up the number of transformations and update the matrix
#tot.trans = rbind(tot.trans,c(dist.unique,nrow(Distance_Results),nrow(Sample_Peak_Mat),nrow(Distance_Results)/nrow(Sample_Peak_Mat)))
#change this - export out one line and use that to assemble the details needed from each sample later
##### write out current tot.trans in case crash
#KL turn this off for now
#  # format the total transformations matrix and write it out
#  tot.trans.out = as.data.frame(tot.trans)
#  colnames(tot.trans.out) = c('sample','total.transformations','num.of.formulas','normalized.trans')
#  tot.trans.out$sample = as.character(tot.trans.out$sample)
#  tot.trans.out$total.transformations = as.numeric(as.character(tot.trans.out$total.transformations))
# # write.csv(tot.trans.out,paste(output_dir,"/",Sample_Name,"_Total_Transformations_Temp.csv", sep=""),quote = F,row.names = F)
#  write.csv(tot.trans.out,paste(Sample_Name,"_Total_Transformations_Temp.csv", sep=""),quote = F,row.names = F)
#
#####
#pulled text that was commented out - in tforms2.R for now
print(dist.unique)
print(date())
}
(nrow(Sample_Peak_Mat >= 2) & nrow(Sample_Peak_Mat) < 5000)
tot.trans
dist.unique
nrow(Distance_Results)
nrow(Sample_Peak_Mat)
nrow(Distance_Results)/nrow(Sample_Peak_Mat)
tot.trans <- c(dist.unique,nrow(Distance_Results),nrow(Sample_Peak_Mat),nrow(Distance_Results)/nrow(Sample_Peak_Mat))
tot.trans
#now write that to a text file
write.csv(tot.trans,paste(out_dir_summary,"Summary_",dist.unique,".csv",sep=""),quote = F,row.names = F)
out_dir_summary = "C:/Users/klongnecker/Documents/Dropbox/XX_DOMsynthesis_GreeceMtg/testing/"
#now write that to a text file
write.csv(tot.trans,paste(out_dir_summary,"Summary_",dist.unique,".csv",sep=""),quote = F,row.names = F)
#now write that to a text file
write.csv(t(tot.trans),paste(out_dir_summary,"Summary_",dist.unique,".csv",sep=""),quote = F,row.names = F)
colnames(tot.trans)
header(tot.trans)
tot.trans
t(tot.trans)
#make this one row, with headers
tot.trans <- t(tot.trans)
tot.trans
colnames(tot.trans)
colnames <- c("dist.unique","nDistance_Results","n_Sample_Peak_Mat","n_ratio")
colnames(tot.trans) <- colnames
tot.trans
#now write that to a text file
write.csv(tot.trans,paste(out_dir_summary,"Summary_",dist.unique,".csv",sep=""),quote = F,row.names = F)
dim(data)
# Load metadata object
file_list <- paste0("samplesToProcess",".txt")
files <- read.table(file = file_list,sep="\t",header=TRUE)
f <- 1
current.sample <- files[f,]
current.sample
# error term
error.term = 0.000010
# matrix to hold total number of transformations for each sample
tot.trans = numeric()
print(date())
one.sample.matrix = cbind(as.numeric(as.character(row.names(data))), data[,which(colnames(data) == current.sample), drop = FALSE]) # "drop = FALSE" ensures that the row and column names remain associated with the data
colnames(one.sample.matrix) = c("peak", colnames(one.sample.matrix[2]))
quit()
